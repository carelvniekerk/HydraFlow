# @package _global_
use_deepspeed: false
use_fsdp: false
use_parallelism_config: false
use_megatron_lm: true
use_xpu: false

# Megatron-LM starter settings
megatron_lm_tp_degree: 2 # Tensor parallel degree (set to fit wide layers)
megatron_lm_pp_degree: 1 # Increase for very deep models to pipeline
megatron_lm_num_micro_batches: null # Set when using pipeline parallel (>= pp_degree * 2)
megatron_lm_sequence_parallelism: null # 'true' to enable sequence/activation partition (after TP stable)
megatron_lm_recompute_activations: null # 'full' or 'selective' for activation checkpointing
megatron_lm_use_distributed_optimizer: null # 'true' to shard optimizer states (later)
megatron_lm_gradient_clipping: 1.0 # Stable default
